name: "Darkent2Caffe"
input: "data"
input_dim: 1
input_dim: 3
input_dim: 384
input_dim: 640

layer {
    bottom: "data"
    top: "layer1-conv"
    name: "layer1-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 2
        bias_term: false
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer1-conv"
    name: "layer1-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer1-conv"
    top: "layer2-maxpool"
    name: "layer2-maxpool"
    type: "Pooling"
    pooling_param {
        stride: 2
        pool: MAX
        kernel_size: 2
        pad: 0
    }
}
layer {
    bottom: "layer2-maxpool"
    top: "layer3-conv"
    name: "layer3-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer3-conv"
    name: "layer3-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer3-conv"
    top: "layer4-conv"
    name: "layer4-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer4-conv"
    top: "layer4-conv"
    name: "layer4-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer2-maxpool"
    bottom: "layer4-conv"
    top: "layer5-shortcut"
    name: "layer5-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer5-shortcut"
    top: "layer5-shortcut"
    name: "layer5-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer5-shortcut"
    top: "layer6-conv"
    name: "layer6-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer6-conv"
    name: "layer6-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer6-conv"
    top: "layer7-conv"
    name: "layer7-conv"
    type: "Convolution"
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer7-conv"
    top: "layer7-conv"
    name: "layer7-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer5-shortcut"
    bottom: "layer7-conv"
    top: "layer8-shortcut"
    name: "layer8-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer8-shortcut"
    top: "layer8-shortcut"
    name: "layer8-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer8-shortcut"
    top: "layer9-maxpool"
    name: "layer9-maxpool"
    type: "Pooling"
    pooling_param {
        stride: 2
        pool: MAX
        kernel_size: 2
        pad: 0
    }
}
layer {
    bottom: "layer9-maxpool"
    top: "layer10-conv"
    name: "layer10-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer10-conv"
    name: "layer10-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer10-conv"
    top: "layer11-conv"
    name: "layer11-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer11-conv"
    name: "layer11-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer11-conv"
    top: "layer12-conv"
    name: "layer12-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer12-conv"
    top: "layer12-conv"
    name: "layer12-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer12-conv"
    top: "layer12-conv"
    name: "layer12-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer10-conv"
    bottom: "layer12-conv"
    top: "layer13-shortcut"
    name: "layer13-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer13-shortcut"
    top: "layer13-shortcut"
    name: "layer13-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer13-shortcut"
    top: "layer14-conv"
    name: "layer14-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer14-conv"
    name: "layer14-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer14-conv"
    top: "layer15-conv"
    name: "layer15-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer15-conv"
    name: "layer15-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer15-conv"
    top: "layer16-conv"
    name: "layer16-conv"
    type: "Convolution"
    convolution_param {
        num_output: 128
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer16-conv"
    top: "layer16-conv"
    name: "layer16-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer16-conv"
    top: "layer16-conv"
    name: "layer16-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer14-conv"
    bottom: "layer16-conv"
    top: "layer17-shortcut"
    name: "layer17-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer17-shortcut"
    top: "layer17-shortcut"
    name: "layer17-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer17-shortcut"
    top: "layer18-maxpool"
    name: "layer18-maxpool"
    type: "Pooling"
    pooling_param {
        stride: 2
        pool: MAX
        kernel_size: 2
        pad: 0
    }
}
layer {
    bottom: "layer18-maxpool"
    top: "layer19-conv"
    name: "layer19-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer19-conv"
    name: "layer19-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer19-conv"
    top: "layer20-conv"
    name: "layer20-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer20-conv"
    name: "layer20-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer20-conv"
    top: "layer21-conv"
    name: "layer21-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer21-conv"
    top: "layer21-conv"
    name: "layer21-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer21-conv"
    top: "layer21-conv"
    name: "layer21-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer19-conv"
    bottom: "layer21-conv"
    top: "layer22-shortcut"
    name: "layer22-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer22-shortcut"
    top: "layer22-shortcut"
    name: "layer22-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer22-shortcut"
    top: "layer23-conv"
    name: "layer23-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer23-conv"
    name: "layer23-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer23-conv"
    top: "layer24-conv"
    name: "layer24-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer24-conv"
    name: "layer24-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer24-conv"
    top: "layer25-conv"
    name: "layer25-conv"
    type: "Convolution"
    convolution_param {
        num_output: 256
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer25-conv"
    top: "layer25-conv"
    name: "layer25-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer25-conv"
    top: "layer25-conv"
    name: "layer25-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer23-conv"
    bottom: "layer25-conv"
    top: "layer26-shortcut"
    name: "layer26-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer26-shortcut"
    top: "layer26-shortcut"
    name: "layer26-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer26-shortcut"
    top: "layer27-maxpool"
    name: "layer27-maxpool"
    type: "Pooling"
    pooling_param {
        stride: 2
        pool: MAX
        kernel_size: 2
        pad: 0
    }
}
layer {
    bottom: "layer27-maxpool"
    top: "layer28-conv"
    name: "layer28-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer28-conv"
    top: "layer28-conv"
    name: "layer28-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer28-conv"
    top: "layer28-conv"
    name: "layer28-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer28-conv"
    top: "layer28-conv"
    name: "layer28-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer28-conv"
    top: "layer29-conv"
    name: "layer29-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer29-conv"
    name: "layer29-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer29-conv"
    name: "layer29-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer29-conv"
    top: "layer30-conv"
    name: "layer30-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer30-conv"
    top: "layer30-conv"
    name: "layer30-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer28-conv"
    bottom: "layer30-conv"
    top: "layer31-shortcut"
    name: "layer31-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer31-shortcut"
    top: "layer31-shortcut"
    name: "layer31-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer31-shortcut"
    top: "layer32-conv"
    name: "layer32-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer32-conv"
    name: "layer32-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer32-conv"
    top: "layer33-conv"
    name: "layer33-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer33-conv"
    name: "layer33-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer33-conv"
    top: "layer34-conv"
    name: "layer34-conv"
    type: "Convolution"
    convolution_param {
        num_output: 512
        kernel_size: 1
        pad: 0
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer34-conv"
    top: "layer34-conv"
    name: "layer34-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer34-conv"
    top: "layer34-conv"
    name: "layer34-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer32-conv"
    bottom: "layer34-conv"
    top: "layer35-shortcut"
    name: "layer35-shortcut"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}
layer {
    bottom: "layer35-shortcut"
    top: "layer35-shortcut"
    name: "layer35-act"
    type: "ReLU"
    relu_param {
        negative_slope: 0.1
    }
}
layer {
    bottom: "layer35-shortcut"
    top: "layer36-conv"
    name: "layer36-conv"
    type: "Convolution"
    convolution_param {
        num_output: 30
        kernel_size: 1
        stride: 1
        bias_term: false
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer36-conv"
    name: "layer36-bn"
    type: "BatchNorm"
    batch_norm_param {
        use_global_stats: true
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer36-conv"
    name: "layer36-scale"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}
layer {
    bottom: "layer36-conv"
    top: "layer37-region"
    name: "layer37-region"
    type: "Region"
    region_param {
        anchors: "0.52,0.61, 1.05,1.12, 1.85,2.05, 4.63,4.49, 7.15,7.56"
        classes: 1
        num: 5
    }
}
